<div align="center">

# HyprVoice

**Voice Control for the Modern Linux Desktop**

[![License: GPL-3.0](https://img.shields.io/badge/License-GPL%203.0-blue.svg)](LICENSE)
[![Arch Linux](https://img.shields.io/badge/Arch_Linux-1793D1?logo=arch-linux&logoColor=white)](https://archlinux.org/)
[![Hyprland](https://img.shields.io/badge/Hyprland-00ADD8?logo=wayland&logoColor=white)](https://hyprland.org/)
[![Version](https://img.shields.io/badge/Version-1.0.0-green.svg)](https://github.com/Sarthak-Mhaske/HyprVoice/releases)

A hybrid voice assistant combining offline wake word detection with cloud-powered  
speech recognition. Built for Arch Linux, Hyprland, and Quickshell.

[Installation](#installation) Â· [Commands](#available-commands) Â· [Configuration](#configuration) Â· [Roadmap](#roadmap)

---

**Also known as:** `hyprland-voice-assistant` Â· `linux-voice-assistant`

</div>

---

## Overview

HyprVoice brings voice control to the Linux desktop. Say "Alexa" to wake it up, speak your command, and watch it executeâ€”whether that's playing music, taking screenshots, adjusting volume, or controlling your windows.

---

## Demo

### ðŸŽ¤ Voice Activation
Say "Alexa" to activate, then give any command:

![Voice Activation Demo](assets/voice-activation.gif)

### ðŸŽµ Music Playback
Control YouTube Music with your voice:

![Music Playback Demo](assets/music-demo.gif)

### ðŸ“¸ Screenshot Capture
Take screenshots using voice commands:

![Screenshot Demo](assets/screenshot-demo.gif)

---

**The hybrid approach:**
- **Local:** Wake word detection and text-to-speech (privacy, zero latency)
- **Cloud:** Speech recognition and language processing (accuracy, 90+ languages)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Voice   â”‚ â”€â”€â–¶ â”‚  Wake    â”‚ â”€â”€â–¶ â”‚  Record  â”‚ â”€â”€â–¶ â”‚   STT    â”‚
â”‚  Input   â”‚     â”‚  (local) â”‚     â”‚  5 sec   â”‚     â”‚  (cloud) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â–¼
â”‚  Speak   â”‚ â—€â”€â”€ â”‚   TTS    â”‚ â—€â”€â”€ â”‚  Execute â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ Response â”‚     â”‚  (local) â”‚     â”‚  Action  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **Privacy:** Audio is only transmitted after wake word detection. Wake word processing happens entirely on-device.

---

> **âš ï¸ Compatibility Notice**  
> This release requires [End-4's illogical-impulse Quickshell configuration](https://github.com/end-4/dots-hyprland).  
> HyprVoice extends their existing `Ai.qml` service and assumes their directory structure.  
>  
> **Not using illogical-impulse?** Support for standalone installation and other configs is planned for v2.0+.

---

## Features

### Voice Pipeline

| Component | Technology | Runs Locally |
|-----------|------------|:------------:|
| Wake Word | OpenWakeWord | âœ“ |
| Speech-to-Text | Groq Whisper API | âœ— |
| Language Model | Groq LLM API | âœ— |
| Text-to-Speech | Piper TTS | âœ“ |

### 19 Function Tools

**Media** â€” play, pause, next, previous, volume, audio output switching  
**System** â€” brightness, screenshots (area/window/screen), notifications, reminders  
**Apps** â€” launch allowlisted applications, open URLs, Google search, YouTube Music  
**Shell** â€” read/write Quickshell config, execute commands (with approval), switch LLM models

### Language Support

English, Hindi (à¤¹à¤¿à¤‚à¤¦à¥€), and Marathi (à¤®à¤°à¤¾à¤ à¥€) with automatic detection.

### Automatic Failover

Rate limited? The system automatically switches models:

```
Llama 3.3 70B â†’ Llama 4 Scout â†’ Maverick â†’ Qwen 3 â†’ Kimi K2 â†’ Llama 3.1 8B
```

---

## Requirements

- **OS:** Arch Linux (or Arch-based)
- **WM:** Hyprland
- **Shell:** Quickshell (End-4/illogical-impulse config)
- **Audio:** PipeWire + WirePlumber
- **API:** Groq account (free tier works)

### Dependencies

```bash
# Core
pacman -S --needed python python-pyaudio jq curl libnotify \
    playerctl brightnessctl wl-clipboard pipewire wireplumber

# AUR
paru -S quickshell-git grimblast-git piper-tts-bin yt-dlp
```

---

## Installation

### Quick Install

```bash
git clone https://github.com/Sarthak-Mhaske/HyprVoice.git
cd HyprVoice
./install.sh
```

### Manual Setup

<details>
<summary>Click to expand</summary>

**1. Copy files**
```bash
cp -r config/quickshell/ii/* ~/.config/quickshell/ii/
cp -r config/hypr/scripts/* ~/.config/hypr/scripts/
cp -r config/hypr/voice-engine/* ~/.config/hypr/voice-engine/
cp -r config/hypr/voice-assistant/* ~/.config/hypr/voice-assistant/
chmod +x ~/.config/hypr/scripts/*.sh
```

**2. Python environment**
```bash
cd ~/.config/hypr/voice-engine
python -m venv venv
./venv/bin/pip install openwakeword pyaudio numpy
```

**3. Piper TTS**
```bash
mkdir -p ~/.local/share/piper/models
cd ~/.local/share/piper
python -m venv venv
./venv/bin/pip install piper-tts
# Download models: en_US-lessac-medium.onnx, hi_IN-pratham-medium.onnx
```

**4. API Key**
```bash
secret-tool store --label='illogical-impulse' application illogical-impulse
# Enter: {"apiKeys": {"groq": "your-key-here"}}
```

**5. Autostart** â€” add to `~/.config/hypr/hyprland/execs.conf`:
```conf
exec-once = qs -n -c ii
exec-once = setsid -f bash ~/.config/hypr/scripts/start-voice-listener.sh
```

</details>

---

## Available Commands

| Category | Examples |
|----------|----------|
| **Media** | "Play music", "Next song", "Set volume to 50", "Play Shape of You" |
| **System** | "Set brightness to 80", "Take a screenshot", "Screenshot this window" |
| **Apps** | "Open Brave", "Open terminal", "Open file manager" |
| **Reminders** | "Remind me in 10 minutes to check email" |
| **Web** | "Search for Linux tutorials", "Open github.com" |
| **Assistant** | "Switch to the fast model", "Use the best model" |

---

## Configuration

### Wake Word

| Setting | Default |
|---------|---------|
| Wake Word | alexa |
| Threshold | 0.5 |
| Sample Rate | 16kHz |

### Music Player

Edit `~/.config/hypr/voice-assistant/music-player.conf`:

```bash
PREFERRED_PLAYER="youtube-music"
OPEN_IN_BACKGROUND="true"
TARGET_WORKSPACE="3"
```

### LLM Models

| Model | Use Case | Daily Limit |
|-------|----------|-------------|
| llama-3.3-70b | Best quality (default) | 1K |
| llama-4-maverick | Complex reasoning | 1K |
| llama-4-scout | Balanced | 1K |
| llama-3.1-8b | Fastest | 14K |
| qwen3-32b | Alternative | 1K |
| kimi-k2 | Backup | 1K |

Switch via voice: *"Switch to the fast model"*  
Switch via IPC: `qs ipc -c ii call ai setModel "groq-llama-3-1-8b"`

---

## Architecture

```
QUICKSHELL
â”œâ”€â”€ Ai.qml              # LLM communication, function tools, IPC
â””â”€â”€ VoiceOverlay.qml    # Visual feedback during listening

VOICE ENGINE
â”œâ”€â”€ listen.py           # Wake word detection (OpenWakeWord)
â”œâ”€â”€ voice-assistant.sh  # STT pipeline (Groq Whisper)
â””â”€â”€ text-to-speech.sh   # TTS output (Piper)

EXTERNAL
â”œâ”€â”€ Groq Whisper API    # Speech-to-text
â””â”€â”€ Groq LLM API        # Intent parsing + responses
```

IPC communication:
```bash
qs ipc -c ii call ai voiceMessage "#voice <transcription>"
```

---

## Troubleshooting

<details>
<summary><b>Listener not starting</b></summary>

```bash
pgrep -f "listen.py"                              # Check if running
tail -f /tmp/voice-listener.log                   # View logs
pkill -f listen.py && bash ~/.config/hypr/scripts/start-voice-listener.sh  # Restart
```
</details>

<details>
<summary><b>No TTS audio</b></summary>

```bash
echo "Hello" | ~/.config/hypr/scripts/text-to-speech.sh  # Test TTS
ls ~/.local/share/piper/models/                          # Check models
```
</details>

<details>
<summary><b>API errors</b></summary>

```bash
secret-tool lookup 'application' 'illogical-impulse'     # Verify key
```
</details>

**Common fixes:**

| Problem | Solution |
|---------|----------|
| No API key error | Store key with secret-tool |
| Wake word not detected | Increase mic gain or reduce threshold to 0.4 |
| Hindi detected as Arabic | Script auto-retries with Hindi hint |
| Rate limit errors | System auto-switches to fallback model |

---

## Roadmap

### v1.x (Current)

- [x] Core voice pipeline
- [x] 19 function tools
- [x] Multi-language support (EN/HI/MR)
- [ ] Spotify integration (v1.1)
- [ ] 30+ app allowlist (v1.2)
- [ ] Window management (v1.3)
- [ ] Power controls â€” lock, suspend, hibernate (v1.4)
- [ ] Network controls â€” WiFi, Bluetooth (v1.5)

### v2.x (Planned)

- [ ] Custom wake words
- [ ] Offline STT (Whisper.cpp) â€” *experimental*
- [ ] Offline LLM (Ollama) â€” *experimental*
- [ ] Plugin system
- [ ] Settings UI

### Provider Support

| Provider | Status |
|----------|--------|
| Groq | âœ“ Supported |
| OpenAI | Planned (v1.1) |
| Anthropic | Planned (v1.1) |
| Google Gemini | Planned (v1.1) |
| Ollama (local) | Planned (v2.0) |

> The function-calling architecture is provider-agnostic. Adding new providers is straightforward.

### Offline Mode

We're exploring fully offline operation for v2.x. Current status:

| Component | v1.0 | v2.x Goal |
|-----------|:----:|:---------:|
| Wake Word | Local | Local |
| STT | Cloud | Local (Whisper.cpp) |
| LLM | Cloud | Local (Ollama) |
| TTS | Local | Local |

> **Note:** Full offline mode requires 8-16GB RAM. Hybrid mode will remain the default.

---

## File Structure

```
~/.config/quickshell/ii/
â”œâ”€â”€ services/Ai.qml
â””â”€â”€ modules/ii/voiceOverlay/VoiceOverlay.qml

~/.config/hypr/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ voice-assistant.sh
â”‚   â”œâ”€â”€ text-to-speech.sh
â”‚   â”œâ”€â”€ start-voice-listener.sh
â”‚   â””â”€â”€ play-music.sh
â”œâ”€â”€ voice-engine/
â”‚   â”œâ”€â”€ listen.py
â”‚   â””â”€â”€ models/alexa.onnx
â””â”€â”€ voice-assistant/
    â””â”€â”€ music-player.conf

~/.local/share/piper/
â””â”€â”€ models/
    â”œâ”€â”€ en_US-lessac-medium.onnx
    â””â”€â”€ hi_IN-pratham-medium.onnx
```

---

## Contributing

```bash
git clone https://github.com/YOUR_USERNAME/HyprVoice.git
git checkout -b feature/your-feature
# Make changes, test
git commit -m "feat: add your feature"
git push origin feature/your-feature
# Open Pull Request
```

**Commit format:** `type: description`  
Types: `feat`, `fix`, `docs`, `refactor`, `chore`

---

## Acknowledgments

### Built On

This project extends the Quickshell configuration from **[End-4's dots-hyprland](https://github.com/end-4/dots-hyprland)** (illogical-impulse). The Ai.qml service and overall shell architecture are built upon their excellent work.

Special thanks to **[@end-4](https://github.com/end-4)** and all [170+ contributors](https://github.com/end-4/dots-hyprland/graphs/contributors) to the dots-hyprland project.

### Technologies

- [OpenWakeWord](https://github.com/dscripka/openWakeWord) â€” Wake word detection
- [Piper](https://github.com/rhasspy/piper) â€” Offline text-to-speech
- [Groq](https://groq.com) â€” Fast LLM and STT inference
- [Hyprland](https://hyprland.org) â€” Wayland compositor
- [Quickshell](https://github.com/quickshell-mirror/quickshell) â€” Qt6/QML shell framework

---

## License

This project is licensed under the **GNU General Public License v3.0** â€” see [LICENSE](LICENSE) for details.

This project builds upon [dots-hyprland](https://github.com/end-4/dots-hyprland) which is also GPL-3.0 licensed.

---

<div align="center">

**[Issues](https://github.com/Sarthak-Mhaske/HyprVoice/issues)** Â· **[Discussions](https://github.com/Sarthak-Mhaske/HyprVoice/discussions)** Â· **[Wiki](https://github.com/Sarthak-Mhaske/HyprVoice/wiki)**

</div>